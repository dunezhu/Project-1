{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOulPVvEqw7yYE3sz2eY9aN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3iSQlKN8JbH"
      },
      "outputs": [],
      "source": [
        "#Install the necessary libraries\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install python-docx\n",
        "!pip install docx2txt\n",
        "\n",
        "# Import the libraries\n",
        "import requests\n",
        "import docx2txt\n",
        "import docx\n",
        "import io\n",
        "\n",
        "from bs4 import BeautifulSoup \n",
        "from google.colab import files  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Word file containing URLs from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the URLs from the uploaded Word file\n",
        "text = docx2txt.process(list(uploaded.keys())[0])\n",
        "urls = [url.strip() for url in text.split('\\n') if url.strip()]\n",
        "\n",
        "# Print the extracted URLs\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "CwCc1w9CK3Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = docx.Document()\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        links = [link.get('href') \n",
        "        \n",
        "        for link in soup.find_all('a')]\n",
        "        print(f\"Linked pages for {url}:\")\n",
        "        \n",
        "        for link in links:\n",
        "            #print(link)\n",
        "            doc.add_paragraph(link)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing {url}: {str(e)}\")\n",
        "\n",
        "# Save the changes to a modified document\n",
        "doc.save('extract.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('extract.docx')           "
      ],
      "metadata": {
        "id": "BQEqGyYrLgYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import document\n",
        "\n",
        "# Select the Word file from your Local Machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Open the Word file\n",
        "doc = docx.Document(list(uploaded.keys())[0]) "
      ],
      "metadata": {
        "id": "ngKTp8Km4GwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [URL SPECIFIC]: Add 'https://www.website.suffix' before all entries beginning in '/' \n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('/'): \n",
        "        para.text = \"https://www.website.suffix\" + para.text "
      ],
      "metadata": {
        "id": "pA5TTgZT-du5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the line does not start with 'http' and delete\n",
        "for para in doc.paragraphs:\n",
        "    if not para.text.startswith(\"http\"):\n",
        "        para.text = \"\" "
      ],
      "metadata": {
        "id": "ienTlD7r-o5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Check if the line does not contains base 'website' and delete\n",
        "for para in doc.paragraphs:\n",
        "   if 'website' not in para.text:\n",
        "      para.text = \"\"  "
      ],
      "metadata": {
        "id": "BcNC5fCbzNwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the line contains '[keywords]' and delete\n",
        "\n",
        "keywords = [ \n",
        "\n",
        "#Social media    \n",
        "'google', 'Google', 'microsoft', 'Microsoft', 'github', 'zoom', 'youtube', 'twitter', 'facebook', 'instagram', 'pinterest', 'tiktok', 'linkedin', 'vimeo', 'slack', '.app',     \n",
        "\n",
        "#File content\n",
        "'pdf', 'doc', 'docx', 'xls', 'xlsx', 'aspx', 'wdt', 'imaging', 'mail', 'apply', 'login', '@', 'map', 'media', 'jpg', 'png', 'gif', '%',  \n",
        "\n",
        "#Website specific\n",
        "'banweb', 'oam', 'canvas', 'my', 'My', '/.', 'pdxscholar', 'banner', 'cconn', 'fast.', 'dillon',  \n",
        "\n",
        "    ]\n",
        "\n",
        "for keyword in keywords:\n",
        "    for para in doc.paragraphs:\n",
        "        if keyword in para.text:\n",
        "           para.text = \"\"       "
      ],
      "metadata": {
        "id": "J7YSBoXaiuBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the paragraphs in the document and delete duplicates\n",
        "# Create a set to store unique lines\n",
        "unique_lines = set()\n",
        "\n",
        "for para in doc.paragraphs:\n",
        "    # If the line is not already in the set, add it\n",
        "    if para.text not in unique_lines:\n",
        "        unique_lines.add(para.text)\n",
        "    # If the line is already in the set, remove it\n",
        "    else:\n",
        "        para.text = \"\"    "
      ],
      "metadata": {
        "id": "cEPU94TBjmeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the changes to a modified document\n",
        "doc.save('output.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('output.docx')     "
      ],
      "metadata": {
        "id": "4TPou6VGgzlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the first word file\n",
        "uploaded_file1 = files.upload()\n",
        "doc1 = docx.Document(io.BytesIO(uploaded_file1['output.docx']))\n",
        "\n",
        "# Upload the second word file\n",
        "uploaded_file2 = files.upload()\n",
        "doc2 = docx.Document(io.BytesIO(uploaded_file2['input.docx']))\n",
        "\n",
        "# Combine the two files\n",
        "doc1.add_page_break()\n",
        "for paragraph in doc2.paragraphs:\n",
        "    doc1.add_paragraph(paragraph.text)\n",
        "\n",
        "# Save the combined file\n",
        "doc1.save(\"product.docx\")\n",
        "\n",
        "# Download the combined file\n",
        "files.download(\"product.docx\")"
      ],
      "metadata": {
        "id": "KHb05sMMi8Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import document\n",
        "\n",
        "# Select the Word file from your Local Machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Open the Word file\n",
        "doc = docx.Document(list(uploaded.keys())[0]) "
      ],
      "metadata": {
        "id": "H55naWOXn276"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the paragraphs in the document and delete duplicates\n",
        "# Create a set to store unique lines\n",
        "unique_lines = set()\n",
        "\n",
        "for para in doc.paragraphs:\n",
        "    # If the line is not already in the set, add it\n",
        "    if para.text not in unique_lines:\n",
        "        unique_lines.add(para.text)\n",
        "    # If the line is already in the set, remove it\n",
        "    else:\n",
        "        para.text = \"\"    "
      ],
      "metadata": {
        "id": "OF6vsJ4-orge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the changes to a modified document\n",
        "doc.save('package.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('package.docx')     "
      ],
      "metadata": {
        "id": "9wemvnAgUsBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Word file containing URLs from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the URLs from the uploaded Word file\n",
        "text = docx2txt.process(list(uploaded.keys())[0])\n",
        "urls = [url.strip() for url in text.split('\\n') if url.strip()]\n",
        "\n",
        "# Print the extracted URLs\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "ieOU-kZ6Uyni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Word file to a Text file and download\n",
        "\n",
        "# Upload the word document from the desktop\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    with open(name, 'wb') as f:\n",
        "        f.write(data)\n",
        "    print(f'File \"{name}\" was imported successfully!')\n",
        "    \n",
        "    # Extract text from the word document\n",
        "    text = docx2txt.process(name)\n",
        "\n",
        "    # Remove blank spaces and newline characters\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        " \n",
        "    # Save the text to a file\n",
        "    with open(name+\".txt\", \"w\") as text_file:\n",
        "        text_file.write(text)\n",
        "        \n",
        "    # Download the text file\n",
        "    files.download(name+\".txt\")"
      ],
      "metadata": {
        "id": "-L40BegOVFzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the text file from local desktop to Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the contents of the uploaded file\n",
        "text = \"\"\n",
        "for filename in uploaded.keys():\n",
        "  text = uploaded[filename].decode('utf-8')\n",
        "\n",
        "# Separate the words in the text file\n",
        "words = text.split()\n",
        "\n",
        "# Convert the words to a Word document\n",
        "doc = docx.Document()\n",
        "for word in words:\n",
        "  doc.add_paragraph(word)\n",
        "\n",
        "# Save the Word document to a file\n",
        "doc_filename = 'finish.docx'\n",
        "doc.save(doc_filename)\n",
        "\n",
        "# Download the Word file to local desktop\n",
        "files.download(doc_filename)"
      ],
      "metadata": {
        "id": "lAQ4jbb46od2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for paragraph in doc.paragraphs:\n",
        "  # Print the text of the paragraph\n",
        "  print(paragraph.text) "
      ],
      "metadata": {
        "id": "6RL33VYqEwqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vet the URL List for errors\n",
        " \n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        # Process the response content here\n",
        "        print(f\"Success: {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {url} - {str(e)}\") "
      ],
      "metadata": {
        "id": "2yvBkMfEtIlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the location where keyword is found\n",
        "def search_keyword(urls, keyword):\n",
        "    for url in urls:\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        count = 0\n",
        "        for text in soup.find_all(text=True):\n",
        "            if keyword in text:\n",
        "                count +=1\n",
        "        if count:\n",
        "            print(f'Keyword found {count} times at {url}')\n",
        "        \n",
        "search_keyword(urls, 'Keyword')  "
      ],
      "metadata": {
        "id": "wzOhN9M9tM8G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}