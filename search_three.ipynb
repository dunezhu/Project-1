{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dunezhu/Project-1/blob/master/search_three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3iSQlKN8JbH"
      },
      "outputs": [],
      "source": [
        "#Install the necessary libraries\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install python-docx\n",
        "!pip install docx2txt\n",
        "\n",
        "# Import the libraries\n",
        "import requests\n",
        "import docx2txt\n",
        "import docx\n",
        "import io\n",
        "\n",
        "from bs4 import BeautifulSoup \n",
        "from google.colab import files  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQEqGyYrLgYG"
      },
      "outputs": [],
      "source": [
        "#Extract linked websites from list of URLs\n",
        "\n",
        "# Upload the Word file containing URLs from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the URLs from the uploaded Word file\n",
        "text = docx2txt.process(list(uploaded.keys())[0])\n",
        "urls = [url.strip() for url in text.split('\\n') if url.strip()]\n",
        "\n",
        "# Print the extracted URLs\n",
        "print(urls)\n",
        "\n",
        "doc = docx.Document()\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        links = [link.get('href') \n",
        "        \n",
        "        for link in soup.find_all('a')]\n",
        "        #print(f\"Linked pages for {url}:\")\n",
        "        \n",
        "        for link in links:\n",
        "            #print(link)\n",
        "            doc.add_paragraph(link)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing {url}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngKTp8Km4GwY"
      },
      "outputs": [],
      "source": [
        "# [URL SPECIFIC]: Add 'https://www.website.suffix' before all entries beginning in '/' \n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('/'): \n",
        "        para.text = \"https://www.website.suffix\" + para.text \n",
        "\n",
        "# Check if the line does not start with 'http' and delete\n",
        "for para in doc.paragraphs:\n",
        "    if not para.text.startswith(\"http\"):\n",
        "        para.text = \"\" \n",
        "\n",
        "### Check if the line does not contains base 'website' and delete\n",
        "for para in doc.paragraphs:\n",
        "   if 'pdx' not in para.text:\n",
        "      para.text = \"\"  \n",
        "\n",
        "# Check if the line contains '[keywords]' and delete\n",
        "\n",
        "keywords = [ \n",
        "\n",
        "#Social media    \n",
        "'google', 'Google', 'microsoft', 'Microsoft', 'github', 'zoom', 'youtube', 'twitter', 'facebook', 'instagram', 'pinterest', 'tiktok', 'linkedin', 'vimeo', 'slack', 'app', 'duo', 'sso', 'ssl',    \n",
        "\n",
        "#File content\n",
        "'pdf', 'doc', 'docx', 'xls', 'xlsx', 'aspx', 'wdt', 'imaging', 'mail', 'apply', 'login', 'signin', '@', 'map', 'media', 'jpg', 'png', 'gif', '%', 'directory', 'portal', 'url', 'html', 'php',  \n",
        "\n",
        "#Website specific\n",
        "\n",
        "#URL Blacklist\n",
        "\n",
        "    ]\n",
        "\n",
        "for keyword in keywords:\n",
        "    for para in doc.paragraphs:\n",
        "        if keyword in para.text:\n",
        "           para.text = \"\"       \n",
        "\n",
        "# Iterate through the paragraphs in the document and delete duplicates\n",
        "# Create a set to store unique lines\n",
        "unique_lines = set()\n",
        "\n",
        "for para in doc.paragraphs:\n",
        "    # If the line is not already in the set, add it\n",
        "    if para.text not in unique_lines:\n",
        "        unique_lines.add(para.text)\n",
        "    # If the line is already in the set, remove it\n",
        "    else:\n",
        "        para.text = \"\"    \n",
        "\n",
        "# Save the changes to a modified document\n",
        "doc.save('2.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('2.docx')     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHb05sMMi8Au"
      },
      "outputs": [],
      "source": [
        "#Combine 2 word files\n",
        "\n",
        "# Upload the first word file\n",
        "uploaded_file1 = files.upload()\n",
        "doc1 = docx.Document(io.BytesIO(uploaded_file1['2.docx']))\n",
        "\n",
        "# Upload the second word file\n",
        "uploaded_file2 = files.upload()\n",
        "doc2 = docx.Document(io.BytesIO(uploaded_file2['1.docx']))\n",
        "\n",
        "# Combine the two files\n",
        "doc1.add_page_break()\n",
        "for paragraph in doc2.paragraphs:\n",
        "    doc1.add_paragraph(paragraph.text)\n",
        "\n",
        "# Save the combined file\n",
        "doc1.save(\"3.docx\")\n",
        "\n",
        "# Download the combined file\n",
        "files.download(\"3.docx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H55naWOXn276"
      },
      "outputs": [],
      "source": [
        "# Iterate through the paragraphs in the document and delete duplicates\n",
        "\n",
        "# Select the Word file from your Local Machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Open the Word file\n",
        "doc = docx.Document(list(uploaded.keys())[0]) \n",
        "\n",
        "# Iterate through the paragraphs in the document and delete duplicates\n",
        "# Create a set to store unique lines\n",
        "unique_lines = set()\n",
        "\n",
        "for para in doc.paragraphs:\n",
        "    # If the line is not already in the set, add it\n",
        "    if para.text not in unique_lines:\n",
        "        unique_lines.add(para.text)\n",
        "    # If the line is already in the set, remove it\n",
        "    else:\n",
        "        para.text = \"\"    \n",
        "\n",
        "# Save the changes to a modified document\n",
        "doc.save('4.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('4.docx')     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieOU-kZ6Uyni"
      },
      "outputs": [],
      "source": [
        "# Convert the Word file to a Text file and download\n",
        "\n",
        "# Upload the word document from the desktop\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    with open(name, 'wb') as f:\n",
        "        f.write(data)\n",
        "    print(f'File \"{name}\" was imported successfully!')\n",
        "    \n",
        "    # Extract text from the word document\n",
        "    text = docx2txt.process(name)\n",
        "\n",
        "    # Remove blank spaces and newline characters\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        " \n",
        "    # Save the text to a file\n",
        "    with open(name+\".txt\", \"w\") as text_file:\n",
        "        text_file.write(text)\n",
        "        \n",
        "    # Download the text file\n",
        "    files.download(name+\".txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAQ4jbb46od2"
      },
      "outputs": [],
      "source": [
        "#Convert the Text file to a Word file and download\n",
        "\n",
        "# Upload the text file from local desktop to Colab\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the contents of the uploaded file\n",
        "text = \"\"\n",
        "for filename in uploaded.keys():\n",
        "  text = uploaded[filename].decode('utf-8')\n",
        "\n",
        "# Separate the words in the text file\n",
        "words = text.split()\n",
        "\n",
        "# Convert the words to a Word document\n",
        "doc = docx.Document()\n",
        "for word in words:\n",
        "  doc.add_paragraph(word)\n",
        "\n",
        "# Save the Word document to a file\n",
        "doc_filename = '5.docx'\n",
        "doc.save(doc_filename)\n",
        "\n",
        "# Download the Word file to local desktop\n",
        "files.download(doc_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the text of the paragraph\n",
        "for paragraph in doc.paragraphs:\n",
        "  print(paragraph.text) "
      ],
      "metadata": {
        "id": "EioekZYJ784o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the Word file containing URLs from your local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the URLs from the uploaded Word file\n",
        "text = docx2txt.process(list(uploaded.keys())[0])\n",
        "urls = [url.strip() for url in text.split('\\n') if url.strip()]\n",
        "\n",
        "# Print the extracted URLs\n",
        "print(urls)\n",
        "\n",
        "# Open the Word file\n",
        "doc = docx.Document(list(uploaded.keys())[0]) "
      ],
      "metadata": {
        "id": "wP9yi5GeXTqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yvBkMfEtIlC"
      },
      "outputs": [],
      "source": [
        "# Vet the URL List for errors\n",
        " \n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        # Process the response content here\n",
        "        print(f\"Success: {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {url} - {str(e)}\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the location where keyword is found\n",
        "def search_keyword(urls, keyword):\n",
        "    for url in urls:\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        count = 0\n",
        "        for string in soup.find_all(string=True):\n",
        "            if keyword in string:\n",
        "                count +=1\n",
        "        if count:\n",
        "            print(f'Keyword found {count} times at {url}')\n",
        "        \n",
        "search_keyword(urls, 'Keyword')  "
      ],
      "metadata": {
        "id": "OePGb7inDHAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the individual URL you want to search\n",
        "url = ['https://www.website.suffix']  \n",
        " \n",
        "for url in url:\n",
        "    # send a GET request to the website\n",
        "    response = requests.get(url)"
      ],
      "metadata": {
        "id": "RXF_GGY0unbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for specific keywords\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "keywords = ['Keyword']\n",
        "for keyword in keywords:\n",
        "    matches = soup.find_all(string=lambda string: keyword in string)\n",
        "    print(f'{len(matches)} matches found for keyword \"{keyword}\":')\n",
        "    for match in matches:\n",
        "        print(match)"
      ],
      "metadata": {
        "id": "eXF3LbYLvGb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find a specific 'kind' of tags on the 'specific page'\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Set the keywords you want to search for\n",
        "keywords = ['']\n",
        "\n",
        "# Search for the keywords in each paragraph\n",
        "for p in paragraphs:\n",
        "  if any(keyword in p.text for keyword in keywords):\n",
        "    print(p.text)"
      ],
      "metadata": {
        "id": "K34LQOGWvMG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyhNRHymuApJJMkljSIqMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}