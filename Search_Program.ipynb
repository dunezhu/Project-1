{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFcJvcJofk4HKMO507A3jj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i83MUJmlXUWW"
      },
      "outputs": [],
      "source": [
        "#Install the necessary libraries\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install python-docx\n",
        "\n",
        "# Import the libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup \n",
        "\n",
        "#Searching a URL LIST for links and nested pages\n",
        "from docx import Document\n",
        "from google.colab import files  \n",
        "\n",
        "# Set URLs you want to crawl separated by a , \n",
        "websites = [\n",
        "\n",
        "'https://www.website.suffix'\n",
        "\n",
        "    ]\n",
        "\n",
        "# Create a new Word document\n",
        "document = Document()\n",
        "\n",
        "# find all <a> tags and print their href\n",
        "for url in websites:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    \n",
        "    print(f'Links found on {url}')\n",
        "    for link in soup.find_all('a'):\n",
        "        document.add_paragraph(link.get('href'))\n",
        "\n",
        "# Save the changes to a modified document\n",
        "document.save('modified.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('modified.docx')         "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import 'modified.docx'  \n",
        "!pip install python-docx\n",
        "import docx\n",
        "from google.colab import files\n",
        "\n",
        "# Select the Word file from your Local Machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Open the Word file\n",
        "doc = docx.Document(list(uploaded.keys())[0]) \n",
        "\n",
        "# [URL SPECIFIC]: Add 'https://www.website.suffix' before all entries beginning in '/' : remove '/' at end of URL if copying over website\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('/') and not para.text.startswith('//'):\n",
        "        para.text = \"https://www.website.suffix\" + para.text\n",
        "\n",
        "### UPDATE & ADD 'https://www.website.suffix/specific-requirement' before all entries beginning in '?' \n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('?'):\n",
        "        para.text = \"https://www.website.suffix/specific-requirement\" + para.text\n",
        "      \n",
        "# Add https: before all entries beginning in //\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('//'):\n",
        "        para.text = \"https:\" + para.text\n",
        "\n",
        "# Check if the line does not start with 'http' and delete\n",
        "for para in doc.paragraphs:\n",
        "    if not para.text.startswith(\"http\"):\n",
        "        para.text = \"\" \n",
        "\n",
        "# Check if the line does not contains base 'website' and delete\n",
        "for para in doc.paragraphs:\n",
        "    if 'website' not in para.text:\n",
        "        para.text = \"\"         \n",
        "\n",
        "# Check if the line contains '[keywords]' and delete\n",
        "\n",
        "keywords = [ \n",
        "\n",
        "#Social media    \n",
        "'google', 'instagram', 'youtube', 'twitter', 'facebook', 'pinterest', 'tiktok', 'linkedin', 'vimeo',   \n",
        "\n",
        "#File content\n",
        "'pdf', 'xlsx',\n",
        "\n",
        "#Website specific\n",
        " \n",
        "    ]\n",
        "\n",
        "for keyword in keywords:\n",
        "    for para in doc.paragraphs:\n",
        "        if keyword in para.text:\n",
        "           para.text = \"\"       \n",
        "\n",
        "# Iterate through the paragraphs in the document and delete duplicates\n",
        "# Create a set to store unique lines\n",
        "unique_lines = set()\n",
        "\n",
        "for para in doc.paragraphs:\n",
        "    # If the line is not already in the set, add it\n",
        "    if para.text not in unique_lines:\n",
        "        unique_lines.add(para.text)\n",
        "    # If the line is already in the set, remove it\n",
        "    else:\n",
        "        para.text = \"\"   \n",
        "\n",
        "# Add ' before all entries beginning in http\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('http'):\n",
        "        para.text = \"'\" + para.text\n",
        "\n",
        "# Add ' after all entries beginning in 'http\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith(\"'http\"):\n",
        "        para.text += \"'\"\n",
        "\n",
        "# [SUBSEQUENT LAYER SPECIFIC] : Define a list of recursive URLs to delete for the [Nth] LAYERS URL LIST SEARCH\n",
        "keywords = [\n",
        "\n",
        "#BASE\n",
        "'https://www.website.suffix' \n",
        "\n",
        "#BASE +1\n",
        "'https://www.website.suffix/page1' \n",
        "\n",
        "#BASE +2\n",
        "'https://www.website.suffix/page2'  \n",
        " \n",
        "     ]\n",
        "\n",
        "# Iterate through the paragraphs of the document\n",
        "for para in doc.paragraphs:\n",
        "    for keyword in keywords:\n",
        "        if keyword in para.text:\n",
        "            # If the keyword is found in the paragraph, remove it\n",
        "            para.text = para.text.replace(keyword, \"\")\n",
        "\n",
        "# Save the changes to a modified document\n",
        "doc.save('modified.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('modified.docx')      \n",
        "\n",
        "#RENAME 'modified.docx' to new name"
      ],
      "metadata": {
        "id": "f4aIqg9hX2ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add , after all entries beginning in '\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith(\"'\"):\n",
        "        para.text += \",\"\n",
        "\n",
        "# Save the changes to a modified document\n",
        "doc.save('modified.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('modified.docx') "
      ],
      "metadata": {
        "id": "ywdcU6CFYsq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Word file to a Text file and download\n",
        "!pip install docx2txt\n",
        "from google.colab import files\n",
        "import docx2txt\n",
        "\n",
        "# Upload the word document from the desktop\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    with open(name, 'wb') as f:\n",
        "        f.write(data)\n",
        "    print(f'File \"{name}\" was imported successfully!')\n",
        "    \n",
        "    # Extract text from the word document\n",
        "    text = docx2txt.process(name)\n",
        "\n",
        "    # Remove blank spaces and newline characters\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    # Save the text to a file\n",
        "    with open(name+\".txt\", \"w\") as text_file:\n",
        "        text_file.write(text)\n",
        "        \n",
        "    # Download the text file\n",
        "    files.download(name+\".txt\")"
      ],
      "metadata": {
        "id": "gOCaE9wkY2IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [alpha.ipynb] Import 'renamed' \n",
        "\n",
        "!pip install python-docx\n",
        "import docx\n",
        "from google.colab import files\n",
        "\n",
        "# Select the Word file from your Local Machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Open the Word file\n",
        "doc = docx.Document(list(uploaded.keys())[0]) \n",
        "\n",
        "# Add '\"' to all entries beginning in \"'\"\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith(\"'\"):\n",
        "        para.text = '\"' + para.text\n",
        "\n",
        "# Add '\"' after all entries beginning in '\"'\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('\"'):\n",
        "        para.text += '\"' \n",
        "\n",
        "# Add ',' after all entries beginning in '\"'\n",
        "for para in doc.paragraphs:\n",
        "    if para.text.startswith('\"'):\n",
        "        para.text += \",\"         \n",
        "\n",
        "# Save the changes to a modified document\n",
        "doc.save('modified.docx')\n",
        "\n",
        "# Download the modified Word file\n",
        "files.download('modified.docx')         "
      ],
      "metadata": {
        "id": "Ze2BBN-tY99l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Word file to a Text file and download\n",
        "!pip install docx2txt\n",
        "from google.colab import files\n",
        "import docx2txt\n",
        "\n",
        "# Upload the word document from the desktop\n",
        "uploaded = files.upload()\n",
        "\n",
        "for name, data in uploaded.items():\n",
        "    with open(name, 'wb') as f:\n",
        "        f.write(data)\n",
        "    print(f'File \"{name}\" was imported successfully!')\n",
        "    \n",
        "    # Extract text from the word document\n",
        "    text = docx2txt.process(name)\n",
        "\n",
        "    # Remove blank spaces and newline characters\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    # Save the text to a file\n",
        "    with open(name+\".txt\", \"w\") as text_file:\n",
        "        text_file.write(text)\n",
        "        \n",
        "    # Download the text file\n",
        "    files.download(name+\".txt\")"
      ],
      "metadata": {
        "id": "egQlnrpOZnd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vet the URL List for errors\n",
        " \n",
        "# List of URLs to crawl \n",
        "urls = [\n",
        " \n",
        "'https://www.website.suffix','https://www.website.suffix/page1', 'https://www.website.suffix/page2'  \n",
        "\n",
        "      ]\n",
        "\n",
        "for url in urls:\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        # Process the response content here\n",
        "        print(f\"Success: {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {url} - {str(e)}\") "
      ],
      "metadata": {
        "id": "jw6UZwFpbA-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the location where keyword is found\n",
        "def search_keyword(urls, keyword):\n",
        "    for url in urls:\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        count = 0\n",
        "        for text in soup.find_all(text=True):\n",
        "            if keyword in text:\n",
        "                count +=1\n",
        "        if count:\n",
        "            print(f'Keyword found {count} times at {url}')\n",
        "        \n",
        "search_keyword(urls, 'keyword')  "
      ],
      "metadata": {
        "id": "FjxbT5LnbFxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}